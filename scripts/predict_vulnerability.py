from openai import OpenAI
import json
import os

# Set your OpenAI API key here
client = OpenAI(
  api_key=os.environ.get("OPENAI_API_KEY")
)

def load_prompts(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def send_prompt_to_gpt4(prompt):
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            # model="gpt-3.5-turbo",
            messages=[
                # {"role": "system", "content": "You are an Solidity Auditor."},
                {"role": "user", "content": prompt}
            ],
            # LLM Configurations
            max_tokens=150,
            n=1,
            stop=None,
            temperature=0.5
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error sending prompt to GPT-4: {e}")
        return None
def process_prompts_and_predict(input_file, output_file):
    prompts_data = load_prompts(input_file)
    results = []

    for entry in prompts_data:
        prompt = entry["prompt"]
        file_name = entry["file"]
        folder = entry["folder"]

        print(f"Processing file: {file_name} in folder: {folder}")
        prediction = send_prompt_to_gpt4(prompt)

        print(f"Prediction: {prediction}")
        if prediction:
            results.append({
                "file": file_name,
                "folder": folder,
                "prompt": prompt,
                "prediction": prediction,
                "annotations": entry["annotations"]
            })

        

    with open(output_file, 'w', encoding='utf-8') as out_file:
        json.dump(results, out_file, indent=4)

# Usage
input_file = 'prompts.json'  # The file containing the generated prompts
output_file = 'predictions.json'  # The file where predictions will be saved



process_prompts_and_predict(input_file, output_file)
